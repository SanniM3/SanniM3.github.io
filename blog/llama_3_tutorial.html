<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />

  <title>Implementing LLaMA 3 from Scratch</title>
  <meta name="description" content="A complete guide to building LLaMA 3 from the ground up using PyTorch, covering RMSNorm, RoPE, Grouped Query Attention, and SwiGLU." />

  <!-- Favicon -->
  <link rel="shortcut icon" href="../assets/images/favicon.ico" type="image/x-icon" />

  <!-- Site CSS -->
  <link rel="stylesheet" href="../assets/css/style.css" />

  <!-- Google Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet" />

  <!-- Prism.js for syntax highlighting -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />

  <!-- MathJax for equations -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
    /* Code block styling */
    pre[class*="language-"] {
      background: #1a1a2e !important;
      border-radius: 12px;
      padding: 20px 24px;
      margin: 20px 0 28px 0;
      overflow-x: auto;
      border: 1px solid rgba(255, 255, 255, 0.08);
      box-shadow: 0 4px 20px rgba(0, 0, 0, 0.3);
    }

    code[class*="language-"] {
      font-family: 'JetBrains Mono', 'Fira Code', monospace !important;
      font-size: 0.88rem;
      line-height: 1.6;
    }

    /* Inline code */
    :not(pre) > code {
      background: rgba(255, 255, 255, 0.08);
      padding: 2px 8px;
      border-radius: 4px;
      font-family: 'JetBrains Mono', monospace;
      font-size: 0.9em;
      color: #f8d06b;
    }

    /* Section spacing */
    .blog-content h3 {
      margin-top: 40px;
    }

    .blog-content h4 {
      margin-top: 28px;
      color: rgba(255, 255, 255, 0.9);
    }

    /* Math display */
    .math-block {
      background: rgba(255, 255, 255, 0.03);
      padding: 16px 20px;
      border-radius: 8px;
      margin: 16px 0;
      overflow-x: auto;
    }

    /* Lists */
    .blog-content ul, .blog-content ol {
      padding-left: 24px;
      margin: 12px 0;
    }

    .blog-content li {
      margin: 8px 0;
      color: rgba(255, 255, 255, 0.8);
    }

    /* Callout boxes */
    .callout {
      background: rgba(255, 218, 121, 0.08);
      border-left: 3px solid #f8d06b;
      padding: 16px 20px;
      border-radius: 0 8px 8px 0;
      margin: 20px 0;
    }

    .callout-title {
      font-weight: 600;
      color: #f8d06b;
      margin-bottom: 8px;
    }

    /* Table styling */
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 20px 0;
      background: rgba(255, 255, 255, 0.03);
      border-radius: 8px;
      overflow: hidden;
    }

    th, td {
      padding: 12px 16px;
      text-align: left;
      border-bottom: 1px solid rgba(255, 255, 255, 0.08);
    }

    th {
      background: rgba(255, 255, 255, 0.05);
      font-weight: 600;
      color: rgba(255, 255, 255, 0.9);
    }

    td {
      color: rgba(255, 255, 255, 0.7);
    }

    /* Architecture diagram */
    .architecture-diagram {
      background: #1a1a2e;
      padding: 24px;
      border-radius: 12px;
      font-family: 'JetBrains Mono', monospace;
      font-size: 0.85rem;
      line-height: 1.4;
      color: #8be9fd;
      overflow-x: auto;
      margin: 20px 0;
      white-space: pre;
    }

    /* Strong emphasis */
    .blog-text strong {
      color: rgba(255, 255, 255, 0.95);
    }
  </style>
</head>
<body>

  <main>
    <div class="main-content">

      <!-- Navigation -->
      <nav class="navbar">
        <ul class="navbar-list">
          <li class="navbar-item">
            <a class="navbar-link" href="../index.html">← Back to Home</a>
          </li>
          <li class="navbar-item">
            <a class="navbar-link" href="../index.html#blog">Blog</a>
          </li>
        </ul>
      </nav>

      <article class="blog active" data-page="blog">

        <header>
          <h2 class="h2 article-title">
            Implementing LLaMA 3 from Scratch: A Complete Guide
          </h2>
        </header>

        <figure class="blog-banner-box">
          <img src="../assets/images/lora.png"
               alt="LLaMA 3 Implementation"
               loading="lazy"
               style="width: 100%; height: 260px; object-fit: cover; border-radius: 16px;">
        </figure>

        <div class="blog-meta" style="margin-top: 10px;">
          <p class="blog-category">Paper Reimplementation</p>
        </div>

        <section class="blog-posts">
          <div class="blog-content">

            <p class="blog-text">
              There's something deeply satisfying about building a language model from scratch. You get to see exactly how the magic happens—no hidden abstractions, no mystery layers. In this tutorial, we're going to build LLaMA 3 piece by piece using PyTorch. By the end, you'll have a working implementation and, more importantly, you'll actually understand what each component does and why it's there.
            </p>

            <p class="blog-text">
              Let's get into it.
            </p>

            <h3 class="h3">What Makes LLaMA 3 Special?</h3>

            <p class="blog-text">
              Before we start coding, let's talk about what we're building. LLaMA 3 is Meta's third generation of open-source language models, and it brought some meaningful upgrades:
            </p>

            <ul>
              <li><strong>A much bigger vocabulary</strong> — 128,256 tokens, up from 32K in LLaMA 2. This means the model can represent text more efficiently.</li>
              <li><strong>Grouped Query Attention (GQA)</strong> — a clever trick that makes inference way faster without sacrificing much quality.</li>
              <li><strong>Longer context</strong> — up to 8,192 tokens out of the box (and extendable to 128K with some tweaks).</li>
              <li><strong>Better training data</strong> — 15 trillion tokens, which is just massive.</li>
            </ul>

            <p class="blog-text">
              Here are the key numbers for the 8B parameter version we'll be implementing:
            </p>

            <table>
              <tr><th>Parameter</th><th>Value</th></tr>
              <tr><td>Layers</td><td>32</td></tr>
              <tr><td>Hidden Size</td><td>4096</td></tr>
              <tr><td>Attention Heads</td><td>32</td></tr>
              <tr><td>KV Heads</td><td>8</td></tr>
              <tr><td>Vocabulary Size</td><td>128,256</td></tr>
              <tr><td>FFN Hidden Size</td><td>14,336</td></tr>
              <tr><td>RoPE θ</td><td>500,000</td></tr>
            </table>

            <p class="blog-text">
              Now let's look at the high-level architecture. The flow is actually pretty simple:
            </p>

            <div class="architecture-diagram">Input Tokens
     │
     ▼
┌─────────────────┐
│  Token Embedding │
└────────┬────────┘
         │
         ▼
┌─────────────────────────────────┐
│       Transformer Block ×32      │
│  ┌─────────────────────────────┐ │
│  │  RMSNorm                    │ │
│  │      ↓                      │ │
│  │  Grouped Query Attention    │ │
│  │      ↓ (+ residual)         │ │
│  │  RMSNorm                    │ │
│  │      ↓                      │ │
│  │  SwiGLU FFN                 │ │
│  │      ↓ (+ residual)         │ │
│  └─────────────────────────────┘ │
└────────────┬────────────────────┘
             │
             ▼
      ┌─────────────┐
      │   RMSNorm   │
      └──────┬──────┘
             │
             ▼
      ┌─────────────┐
      │  Linear Head │
      └──────┬──────┘
             │
             ▼
        Output Logits</div>

            <p class="blog-text">
              Alright, let's start building.
            </p>

            <h3 class="h3">Setting Up the Configuration</h3>

            <p class="blog-text">
              First, we need a config class to hold all our hyperparameters. Nothing fancy here—just a clean way to pass around model settings:
            </p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
from dataclasses import dataclass
from typing import Optional, Tuple
import math

@dataclass
class LlamaConfig:
    """Configuration for LLaMA 3 model."""
    dim: int = 4096                    # Hidden dimension
    n_layers: int = 32                 # Number of transformer layers
    n_heads: int = 32                  # Number of attention heads
    n_kv_heads: int = 8                # Number of key-value heads (for GQA)
    vocab_size: int = 128256           # Vocabulary size
    multiple_of: int = 256             # For FFN dimension rounding
    ffn_dim_multiplier: float = 1.3    # FFN dimension multiplier
    norm_eps: float = 1e-5             # RMSNorm epsilon
    rope_theta: float = 500000.0       # RoPE base frequency
    max_seq_len: int = 8192            # Maximum sequence length
    
    def __post_init__(self):
        # Calculate FFN hidden dimension
        # LLaMA uses 8/3 * dim, then rounds to multiple_of
        hidden_dim = int(2 * (4 * self.dim) / 3)
        if self.ffn_dim_multiplier is not None:
            hidden_dim = int(self.ffn_dim_multiplier * hidden_dim)
        self.ffn_dim = self.multiple_of * (
            (hidden_dim + self.multiple_of - 1) // self.multiple_of
        )</code></pre>

            <h3 class="h3">RMSNorm: A Simpler Way to Normalize</h3>

            <p class="blog-text">
              If you've worked with transformers before, you've probably used LayerNorm. LLaMA uses something called <strong>RMSNorm</strong> instead, and the reason is beautifully simple: it's faster.
            </p>

            <p class="blog-text">
              LayerNorm subtracts the mean and divides by the standard deviation. RMSNorm skips the mean entirely and just divides by the root mean square. Turns out, that centering step in LayerNorm doesn't help much, and removing it gives you a 10-15% speedup. When you're doing billions of normalizations during training, that adds up.
            </p>

            <p class="blog-text">
              Here's the math. For an input vector $\mathbf{x}$:
            </p>

            <div class="math-block">
              $$\text{RMSNorm}(\mathbf{x}) = \frac{\mathbf{x}}{\sqrt{\frac{1}{n}\sum_{i=1}^{n} x_i^2 + \epsilon}} \cdot \gamma$$
            </div>

            <p class="blog-text">
              And here's the code:
            </p>

<pre><code class="language-python">class RMSNorm(nn.Module):
    """
    Root Mean Square Layer Normalization.
    
    Why not LayerNorm? RMSNorm skips the mean computation,
    making it ~10-15% faster with no loss in quality.
    """
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))
    
    def _norm(self, x: torch.Tensor) -> torch.Tensor:
        # Compute RMS along the last dimension
        rms = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)
        return x / rms
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # The float() conversion ensures numerical stability
        # when inputs are in float16 or bfloat16
        output = self._norm(x.float()).type_as(x)
        return output * self.weight</code></pre>

            <p class="blog-text">
              Notice that trick at the end—we compute the normalization in float32 even if the input is float16. This prevents numerical issues that can creep in with lower precision.
            </p>

            <h3 class="h3">Rotary Position Embeddings (RoPE)</h3>

            <p class="blog-text">
              Okay, this is where things get interesting. Position embeddings tell the model where each token sits in the sequence. The original transformer just added learned position vectors to the embeddings. That works, but it has a problem: the model can only handle sequences as long as it was trained on.
            </p>

            <p class="blog-text">
              RoPE takes a completely different approach. Instead of <em>adding</em> position information, it <em>rotates</em> the embeddings based on their position. The beautiful thing is that when you compute attention (which is all about dot products), the relative position between tokens naturally emerges from how the rotations interact.
            </p>

            <p class="blog-text">
              Here's the intuition: imagine each pair of dimensions in your embedding as coordinates on a 2D plane. RoPE rotates these coordinates by an angle that depends on the position. When two tokens compute their attention score (via dot product), the rotation encodes how far apart they are.
            </p>

            <p class="blog-text">
              The math looks like this. For position $m$ and dimension pair $(2i, 2i+1)$:
            </p>

            <div class="math-block">
              $$\begin{pmatrix} q'_{2i} \\ q'_{2i+1} \end{pmatrix} = \begin{pmatrix} \cos(m\theta_i) & -\sin(m\theta_i) \\ \sin(m\theta_i) & \cos(m\theta_i) \end{pmatrix} \begin{pmatrix} q_{2i} \\ q_{2i+1} \end{pmatrix}$$
            </div>

            <p class="blog-text">
              Where $\theta_i = \theta^{-2i/d}$ and $\theta$ is the base frequency. LLaMA 3 uses $\theta = 500,000$—much larger than the 10,000 used in LLaMA 2. This larger base lets the model handle longer sequences more gracefully.
            </p>

            <p class="blog-text">
              Here's how we implement it. The trick is to use complex numbers—a rotation in 2D is just multiplication by $e^{i\theta}$:
            </p>

<pre><code class="language-python">def precompute_freqs_cis(
    dim: int,
    max_seq_len: int,
    theta: float = 500000.0
) -> torch.Tensor:
    """
    Precompute the rotation frequencies for all positions.
    
    We store these as complex numbers because rotation in 2D
    is just multiplication by e^{i*theta}.
    """
    # Frequencies for each dimension pair: theta^(-2i/dim)
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))
    
    # Position indices
    t = torch.arange(max_seq_len)
    
    # Outer product: every position × every frequency
    freqs = torch.outer(t, freqs)
    
    # Convert to complex: e^{i * freqs} = cos(freqs) + i*sin(freqs)
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)
    
    return freqs_cis


def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
    """Reshape frequencies for broadcasting with batched tensors."""
    ndim = x.ndim
    shape = [1 if i != 1 and i != ndim - 1 else d for i, d in enumerate(x.shape)]
    shape[-1] = shape[-1] // 2
    return freqs_cis.view(*shape)


def apply_rotary_emb(
    xq: torch.Tensor,
    xk: torch.Tensor,
    freqs_cis: torch.Tensor
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Apply rotary embeddings to queries and keys.
    
    The trick: treat pairs of real numbers as complex numbers,
    multiply by the rotation, convert back to real.
    """
    # Reshape to complex: pairs of reals become single complex numbers
    xq_complex = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))
    xk_complex = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))
    
    # Reshape frequencies for broadcasting
    freqs_cis = reshape_for_broadcast(freqs_cis, xq)
    
    # Apply rotation via complex multiplication, then back to real
    xq_out = torch.view_as_real(xq_complex * freqs_cis).flatten(-2)
    xk_out = torch.view_as_real(xk_complex * freqs_cis).flatten(-2)
    
    return xq_out.type_as(xq), xk_out.type_as(xk)</code></pre>

            <div class="callout">
              <div class="callout-title">Why θ = 500,000?</div>
              The base frequency controls how quickly the rotations "wrap around." A larger θ means slower rotation, which keeps distant positions more distinguishable. LLaMA 3 uses this larger value specifically to support its 8K context window (and beyond with scaling).
            </div>

            <h3 class="h3">Grouped Query Attention (GQA)</h3>

            <p class="blog-text">
              Now we get to the attention mechanism, and this is where LLaMA 3's secret weapon lives: <strong>Grouped Query Attention</strong>.
            </p>

            <p class="blog-text">
              Here's the problem GQA solves. During inference, the model generates one token at a time. Each new token needs to attend to all previous tokens, which means we need to store all the Keys and Values from earlier in the sequence. This is called the KV cache, and it gets <em>huge</em>. For a 32-layer model with 4K context, you're looking at several gigabytes just for the cache.
            </p>

            <p class="blog-text">
              Standard multi-head attention has separate K and V projections for each head. GQA has a brilliant insight: what if multiple query heads shared the same K and V? You'd have fewer K and V vectors to store, shrinking the cache dramatically.
            </p>

            <p class="blog-text">
              LLaMA 3 8B uses 32 query heads but only 8 KV heads. That's a 4:1 ratio—four query heads share each KV head. The KV cache is 4× smaller, inference is faster, and the quality difference is negligible.
            </p>

<pre><code class="language-python">class Attention(nn.Module):
    """
    Grouped Query Attention.
    
    The key insight: we have fewer K,V heads than Q heads.
    Multiple query heads share the same key-value head,
    dramatically reducing the KV cache size during inference.
    """
    def __init__(self, config: LlamaConfig):
        super().__init__()
        self.n_heads = config.n_heads
        self.n_kv_heads = config.n_kv_heads
        self.head_dim = config.dim // config.n_heads
        self.n_rep = self.n_heads // self.n_kv_heads  # How many Q heads share each KV head
        
        # Q gets the full head count, K and V get the reduced count
        self.wq = nn.Linear(config.dim, config.n_heads * self.head_dim, bias=False)
        self.wk = nn.Linear(config.dim, config.n_kv_heads * self.head_dim, bias=False)
        self.wv = nn.Linear(config.dim, config.n_kv_heads * self.head_dim, bias=False)
        self.wo = nn.Linear(config.n_heads * self.head_dim, config.dim, bias=False)
        
    def forward(
        self,
        x: torch.Tensor,
        freqs_cis: torch.Tensor,
        mask: Optional[torch.Tensor] = None,
        cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None
    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:
        batch_size, seq_len, _ = x.shape
        
        # Project to Q, K, V
        xq = self.wq(x).view(batch_size, seq_len, self.n_heads, self.head_dim)
        xk = self.wk(x).view(batch_size, seq_len, self.n_kv_heads, self.head_dim)
        xv = self.wv(x).view(batch_size, seq_len, self.n_kv_heads, self.head_dim)
        
        # Apply rotary embeddings
        xq, xk = apply_rotary_emb(xq, xk, freqs_cis)
        
        # Handle KV cache for inference
        if cache is not None:
            cached_k, cached_v = cache
            xk = torch.cat([cached_k, xk], dim=1)
            xv = torch.cat([cached_v, xv], dim=1)
        new_cache = (xk, xv) if cache is not None else None
        
        # This is the GQA magic: repeat K,V to match the number of Q heads
        xk = self._repeat_kv(xk)
        xv = self._repeat_kv(xv)
        
        # Standard attention from here
        xq = xq.transpose(1, 2)  # (batch, n_heads, seq_len, head_dim)
        xk = xk.transpose(1, 2)
        xv = xv.transpose(1, 2)
        
        scores = torch.matmul(xq, xk.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        if mask is not None:
            scores = scores + mask
        
        attn_weights = F.softmax(scores, dim=-1)
        output = torch.matmul(attn_weights, xv)
        
        # Reshape and project output
        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)
        return self.wo(output), new_cache
    
    def _repeat_kv(self, x: torch.Tensor) -> torch.Tensor:
        """
        Repeat KV heads to match the number of query heads.
        
        If we have 8 KV heads and 32 Q heads, each KV head
        gets repeated 4 times.
        """
        if self.n_rep == 1:
            return x
        
        batch, seq_len, n_kv_heads, head_dim = x.shape
        x = x[:, :, :, None, :].expand(batch, seq_len, n_kv_heads, self.n_rep, head_dim)
        return x.reshape(batch, seq_len, n_kv_heads * self.n_rep, head_dim)</code></pre>

            <p class="blog-text">
              That <code>_repeat_kv</code> method is doing the heavy lifting for GQA. It takes our 8 KV heads and tiles them to create 32 "virtual" heads that align with our queries. Simple, but incredibly effective.
            </p>

            <h3 class="h3">SwiGLU: A Better Feed-Forward Network</h3>

            <p class="blog-text">
              Every transformer has a feed-forward network (FFN) after attention. The original transformer used ReLU. GPT-2 switched to GELU. LLaMA uses something called <strong>SwiGLU</strong>, and it's a meaningful upgrade.
            </p>

            <p class="blog-text">
              The idea is to add a "gate" that controls information flow. Instead of just activating and projecting, SwiGLU has one path that computes the activation and another path that decides how much of it to let through:
            </p>

            <div class="math-block">
              $$\text{SwiGLU}(\mathbf{x}) = \text{SiLU}(W_1 \mathbf{x}) \odot (W_3 \mathbf{x})$$
            </div>

            <p class="blog-text">
              Here, SiLU is the Sigmoid Linear Unit ($x \cdot \sigma(x)$), and $\odot$ is element-wise multiplication. The $W_3$ projection acts as a learned gate.
            </p>

<pre><code class="language-python">class FeedForward(nn.Module):
    """
    SwiGLU Feed-Forward Network.
    
    The gating mechanism (w3) learns to selectively pass information,
    which empirically outperforms simple ReLU or GELU activations.
    """
    def __init__(self, config: LlamaConfig):
        super().__init__()
        hidden_dim = config.ffn_dim
        
        self.w1 = nn.Linear(config.dim, hidden_dim, bias=False)  # Up projection
        self.w2 = nn.Linear(hidden_dim, config.dim, bias=False)  # Down projection
        self.w3 = nn.Linear(config.dim, hidden_dim, bias=False)  # Gate projection
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # SiLU activation gated by w3
        return self.w2(F.silu(self.w1(x)) * self.w3(x))</code></pre>

            <p class="blog-text">
              Yes, we have three matrices instead of two. The extra parameters are worth it—SwiGLU consistently outperforms simpler activations in language modeling benchmarks.
            </p>

            <h3 class="h3">The Transformer Block</h3>

            <p class="blog-text">
              Now let's put attention and the FFN together into a transformer block. LLaMA uses <strong>pre-normalization</strong>—we normalize <em>before</em> each sub-layer, not after. This makes training more stable, especially for deep models.
            </p>

<pre><code class="language-python">class TransformerBlock(nn.Module):
    """
    A single transformer block with pre-normalization.
    
    The pattern: norm -> sublayer -> residual, repeated twice.
    Pre-norm (as opposed to post-norm) helps with training stability.
    """
    def __init__(self, config: LlamaConfig):
        super().__init__()
        self.attention = Attention(config)
        self.feed_forward = FeedForward(config)
        self.attention_norm = RMSNorm(config.dim, config.norm_eps)
        self.ffn_norm = RMSNorm(config.dim, config.norm_eps)
    
    def forward(
        self,
        x: torch.Tensor,
        freqs_cis: torch.Tensor,
        mask: Optional[torch.Tensor] = None,
        cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None
    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:
        # Attention block with residual
        attn_out, new_cache = self.attention(
            self.attention_norm(x), freqs_cis, mask, cache
        )
        x = x + attn_out
        
        # FFN block with residual
        x = x + self.feed_forward(self.ffn_norm(x))
        
        return x, new_cache</code></pre>

            <h3 class="h3">The Complete Model</h3>

            <p class="blog-text">
              Finally, let's assemble everything into the full LLaMA model:
            </p>

<pre><code class="language-python">class Llama(nn.Module):
    """
    The complete LLaMA 3 model.
    
    Architecture: Embedding -> N × TransformerBlock -> RMSNorm -> Linear
    """
    def __init__(self, config: LlamaConfig):
        super().__init__()
        self.config = config
        
        self.tok_embeddings = nn.Embedding(config.vocab_size, config.dim)
        self.layers = nn.ModuleList([
            TransformerBlock(config) for _ in range(config.n_layers)
        ])
        self.norm = RMSNorm(config.dim, config.norm_eps)
        self.output = nn.Linear(config.dim, config.vocab_size, bias=False)
        
        # Precompute RoPE frequencies
        self.register_buffer(
            "freqs_cis",
            precompute_freqs_cis(
                config.dim // config.n_heads,
                config.max_seq_len,
                config.rope_theta
            )
        )
    
    def forward(
        self,
        tokens: torch.Tensor,
        start_pos: int = 0,
        cache: Optional[list] = None
    ) -> Tuple[torch.Tensor, Optional[list]]:
        batch_size, seq_len = tokens.shape
        
        # Get embeddings
        h = self.tok_embeddings(tokens)
        
        # Get RoPE frequencies for current positions
        freqs_cis = self.freqs_cis[start_pos:start_pos + seq_len]
        
        # Causal mask (only needed for sequences > 1 token)
        mask = None
        if seq_len > 1:
            mask = torch.triu(
                torch.full((seq_len, seq_len), float("-inf"), device=tokens.device),
                diagonal=1
            ).unsqueeze(0).unsqueeze(0)
        
        # Forward through all transformer blocks
        new_cache = [] if cache is not None else None
        for i, layer in enumerate(self.layers):
            layer_cache = cache[i] if cache is not None else None
            h, updated_cache = layer(h, freqs_cis, mask, layer_cache)
            if new_cache is not None:
                new_cache.append(updated_cache)
        
        # Final norm and projection to vocabulary
        h = self.norm(h)
        logits = self.output(h)
        
        return logits, new_cache</code></pre>

            <h3 class="h3">Generating Text</h3>

            <p class="blog-text">
              A language model isn't much fun if we can't generate text with it. Let's add a generate method that uses the KV cache for efficient autoregressive generation:
            </p>

<pre><code class="language-python">@torch.inference_mode()
def generate(
    self,
    prompt_tokens: torch.Tensor,
    max_new_tokens: int = 100,
    temperature: float = 0.8,
    top_p: float = 0.9
) -> torch.Tensor:
    """
    Generate tokens autoregressively.
    
    Uses KV caching so we only compute attention for new tokens,
    not the entire sequence each time.
    """
    batch_size, prompt_len = prompt_tokens.shape
    device = prompt_tokens.device
    
    # Process the entire prompt at once (prefill)
    logits, cache = self.forward(prompt_tokens)
    
    generated = prompt_tokens.tolist()
    next_token_logits = logits[:, -1, :]
    
    for i in range(max_new_tokens):
        # Apply temperature
        next_token_logits = next_token_logits / temperature
        
        # Top-p (nucleus) sampling
        sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
        
        # Remove tokens above the threshold
        sorted_indices_to_remove = cumulative_probs > top_p
        sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()
        sorted_indices_to_remove[:, 0] = False
        
        indices_to_remove = sorted_indices_to_remove.scatter(
            1, sorted_indices, sorted_indices_to_remove
        )
        next_token_logits[indices_to_remove] = float("-inf")
        
        # Sample
        probs = F.softmax(next_token_logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)
        
        for j in range(batch_size):
            generated[j].append(next_token[j].item())
        
        # Forward just the new token, using the cache
        logits, cache = self.forward(
            next_token,
            start_pos=prompt_len + i,
            cache=cache
        )
        next_token_logits = logits[:, -1, :]
    
    return torch.tensor(generated, device=device)</code></pre>

            <p class="blog-text">
              The key insight here is that after processing the prompt, we only pass one token at a time to the model. The KV cache stores all the information from previous tokens, so we don't need to recompute their representations.
            </p>

            <h3 class="h3">Loading Pretrained Weights</h3>

            <p class="blog-text">
              If you want to use Meta's pretrained weights, you'll need to map their naming conventions to ours. Here's a utility function that handles that:
            </p>

<pre><code class="language-python">def load_llama_weights(model: Llama, checkpoint_path: str):
    """
    Load official LLaMA weights into our model.
    
    Meta uses slightly different names, so we map them.
    """
    checkpoint = torch.load(checkpoint_path, map_location="cpu")
    
    key_mapping = {
        "model.embed_tokens.weight": "tok_embeddings.weight",
        "model.norm.weight": "norm.weight",
        "lm_head.weight": "output.weight",
    }
    
    for i in range(model.config.n_layers):
        old = f"model.layers.{i}"
        new = f"layers.{i}"
        
        key_mapping.update({
            f"{old}.self_attn.q_proj.weight": f"{new}.attention.wq.weight",
            f"{old}.self_attn.k_proj.weight": f"{new}.attention.wk.weight",
            f"{old}.self_attn.v_proj.weight": f"{new}.attention.wv.weight",
            f"{old}.self_attn.o_proj.weight": f"{new}.attention.wo.weight",
            f"{old}.mlp.gate_proj.weight": f"{new}.feed_forward.w1.weight",
            f"{old}.mlp.down_proj.weight": f"{new}.feed_forward.w2.weight",
            f"{old}.mlp.up_proj.weight": f"{new}.feed_forward.w3.weight",
            f"{old}.input_layernorm.weight": f"{new}.attention_norm.weight",
            f"{old}.post_attention_layernorm.weight": f"{new}.ffn_norm.weight",
        })
    
    state_dict = {key_mapping[k]: v for k, v in checkpoint.items() if k in key_mapping}
    model.load_state_dict(state_dict)
    print(f"Loaded weights from {checkpoint_path}")</code></pre>

            <h3 class="h3">Putting It All Together</h3>

            <p class="blog-text">
              Here's how you'd use everything:
            </p>

<pre><code class="language-python"># Create the configuration
config = LlamaConfig(
    dim=4096,
    n_layers=32,
    n_heads=32,
    n_kv_heads=8,
    vocab_size=128256,
    rope_theta=500000.0,
    max_seq_len=8192
)

# Create the model
model = Llama(config)

# Check parameter count
total_params = sum(p.numel() for p in model.parameters())
print(f"Total parameters: {total_params:,}")  # ~8 billion

# Move to GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# For inference with pretrained weights:
# load_llama_weights(model, "path/to/checkpoint.pt")
# 
# tokens = tokenizer.encode("Once upon a time", return_tensors="pt").to(device)
# output = model.generate(tokens, max_new_tokens=50)
# print(tokenizer.decode(output[0]))</code></pre>

            <h3 class="h3">Wrapping Up</h3>

            <p class="blog-text">
              And that's LLaMA 3 from scratch! Let's recap what makes this architecture work:
            </p>

            <ul>
              <li><strong>RMSNorm</strong> gives us faster normalization without sacrificing quality</li>
              <li><strong>RoPE</strong> elegantly encodes position through rotation, enabling flexible sequence lengths</li>
              <li><strong>GQA</strong> slashes the KV cache size, making inference dramatically faster</li>
              <li><strong>SwiGLU</strong> adds learnable gating to the FFN for better expressiveness</li>
            </ul>

            <p class="blog-text">
              Each of these choices might seem small in isolation, but together they add up to a model that's both more capable and more efficient than its predecessors.
            </p>

            <p class="blog-text">
              If you want to go deeper, here are some natural next steps:
            </p>

            <ul>
              <li>Implement <strong>Flash Attention</strong> for memory-efficient training</li>
              <li>Add <strong>tensor parallelism</strong> to run the model across multiple GPUs</li>
              <li>Try <strong>speculative decoding</strong> to speed up generation</li>
              <li>Experiment with different <strong>RoPE scaling methods</strong> (NTK-aware, YaRN) for even longer contexts</li>
            </ul>

            <p class="blog-text">
              Happy building!
            </p>

            <hr style="margin: 40px 0; border: none; border-top: 1px solid rgba(255,255,255,0.1);" />

            <h3 class="h3">References</h3>
            <ol class="blog-text" style="padding-left: 18px;">
              <li><a href="https://arxiv.org/abs/2407.21783" target="_blank" rel="noopener">LLaMA 3 Technical Report — Meta AI</a></li>
              <li><a href="https://arxiv.org/abs/2104.09864" target="_blank" rel="noopener">RoFormer: Enhanced Transformer with Rotary Position Embedding — Su et al.</a></li>
              <li><a href="https://arxiv.org/abs/2002.05202" target="_blank" rel="noopener">GLU Variants Improve Transformer — Shazeer</a></li>
              <li><a href="https://arxiv.org/abs/2305.13245" target="_blank" rel="noopener">GQA: Training Generalized Multi-Query Transformer Models — Ainslie et al.</a></li>
              <li><a href="https://arxiv.org/abs/1910.07467" target="_blank" rel="noopener">Root Mean Square Layer Normalization — Zhang & Sennrich</a></li>
            </ol>

          </div>
        </section>

        <div style="margin-top: 24px;">
          <a class="navbar-link" href="../index.html#blog">← Back to Blog</a>
        </div>

      </article>
    </div>
  </main>

  <!-- Site JS -->
  <script src="../assets/js/script.js"></script>

  <!-- Prism.js for syntax highlighting -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>

  <!-- Ionicons -->
  <script type="module" src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.esm.js"></script>
  <script nomodule src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.js"></script>
</body>
</html>
